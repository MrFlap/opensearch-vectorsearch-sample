{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Require python 3.10+\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir opensearch-py python-dotenv boto3 tqdm h5py matplotlib ipywidgets jedi ipython matplotlib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autocomplete use shift+tab\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sift-128 dataset\n",
    "!curl -o sift-128-euclidean.hdf5 -L http://ann-benchmarks.com/sift-128-euclidean.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read Data set\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "dataset = h5py.File('sift-128-euclidean.hdf5', \"r\")\n",
    "X_TRAIN = np.array(dataset[\"train\"])\n",
    "X_TEST = np.array(dataset[\"test\"])\n",
    "dimension = int(dataset.attrs[\"dimension\"]) if \"dimension\" in dataset.attrs else len(X_TRAIN[0])\n",
    "\n",
    "print(f\"Ingest dataset size is : {len(X_TRAIN)}\")\n",
    "print(f\"Queries dataset size is : {len(X_TEST)}\")\n",
    "print(f\"dataset dimensions is : {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "\n",
    "\n",
    "res = load_dotenv(\"environment.txt\")\n",
    "\n",
    "OS_HOST = os.getenv('OS_HOST')\n",
    "OS_PORT = os.getenv('OS_PORT')\n",
    "OS_USER = os.getenv('USER_NAME')\n",
    "OS_PASSWORD = os.getenv('PASSWORD')\n",
    "vector_index_name = os.getenv('VECTOR_INDEX_NAME', \"test-vector\")\n",
    "\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': OS_HOST, 'port': OS_PORT}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (OS_USER, OS_PASSWORD),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    timeout=6000,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "client.info()\n",
    "\n",
    "print(f\"vector index name from env is : {vector_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_index(index_name):\n",
    "    index_mappings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 128, # Adjust to improve precision. Higher improves recall & precsion but increases latency. Lower degrades recall & precision but improves latency.\n",
    "            \"refresh_interval\" : \"-1\" # This is to ensure that we are creating mininal number of segments.\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"vec\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": dimension,\n",
    "                    \"index\": \"true\",\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"space_type\": \"l2\", # l2 for SIFT, cosinesimil for typical\n",
    "                        \"engine\": \"nmslib\",\n",
    "                        \"parameters\": {\n",
    "                            \"ef_construction\": 128\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if client.indices.exists(index=index_name):\n",
    "        response = client.indices.delete(index=index_name)\n",
    "        print(f\"Deleting the index. Response : {response}\")\n",
    "\n",
    "    response = client.indices.create(index=index_name, body=index_mappings)\n",
    "    print(f\"Creating the index. Response : {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest data in the index\n",
    "from tqdm.notebook import tqdm\n",
    "from opensearchpy.helpers import bulk\n",
    "import time\n",
    "\n",
    "\n",
    "create_index(vector_index_name)\n",
    "\n",
    "bulk_size = 1000\n",
    "def dataGen():\n",
    "    for i, vec in enumerate(X_TRAIN):\n",
    "        yield { \"_index\": vector_index_name, \"_id\": str(i + 1), \"vec\": vec.tolist() }\n",
    "\n",
    "data_to_ingest = []\n",
    "total_time_to_ingest = 0.\n",
    "ingest_latency = []\n",
    "for data in tqdm(dataGen(), total=len(X_TRAIN)):\n",
    "    if len(data_to_ingest) == bulk_size:\n",
    "        start = time.time()\n",
    "        (res, errors) = bulk(client, data_to_ingest)\n",
    "        end = time.time()\n",
    "        total_time_to_ingest += (end-start)\n",
    "        ingest_latency.append(end-start)\n",
    "        if len(errors) != 0:\n",
    "            print(errors)\n",
    "            data_to_ingest = []\n",
    "            StopIteration\n",
    "        else:\n",
    "            data_to_ingest = []\n",
    "\n",
    "    if len(data_to_ingest) < bulk_size:\n",
    "        data_to_ingest.append(data)\n",
    "    \n",
    "\n",
    "if len(data_to_ingest) != 0:\n",
    "    start = time.time()\n",
    "    (_, errors) = bulk(client, data_to_ingest)\n",
    "    end = time.time()\n",
    "    total_time_to_ingest += (end-start)\n",
    "    if len(errors) != 0:\n",
    "        print(errors)\n",
    "    else:\n",
    "        data_to_ingest = []\n",
    "\n",
    "print(f\"Ingestion completed. Total time to ingest = {total_time_to_ingest} seconds, average time per document: {total_time_to_ingest/(len(X_TRAIN))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the index as we set the refresh interval to -1\n",
    "client.indices.refresh(index=vector_index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check index details, you should see 1M documents in the index.\n",
    "print(client.cat.indices(index=vector_index_name))\n",
    "\n",
    "print(\"Segments Info After refresh...\")\n",
    "\n",
    "segments = client.cat.segments(vector_index_name, params={\"format\": \"json\"})\n",
    "\n",
    "print(f\"Total segments are: {len(segments)}\")\n",
    "\n",
    "print(f\"Printing Segment info : \\n{client.cat.segments(index=vector_index_name, params={'format': 'csv', 'v': 'true'})}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code to optimize the search latency\n",
    "# Run few queries to load the data in cache. We are running 10 queries here to warmup the cluster\n",
    "# Read more ways to tune the cluster here: https://opensearch.org/docs/latest/search-plugins/knn/performance-tuning/\n",
    "\n",
    "# X_WARMUP = X_TEST[0:10]\n",
    "# for query in tqdm(searchQueryGen(X_WARMUP), total=len(X_WARMUP)):\n",
    "#     search_response = client.search(body=query, index=vector_index_name, _source=False, docvalue_fields=[\"_id\"], stored_fields=\"_none_\")\n",
    "\n",
    "# print(\"--- Warmuped up the Cluster----\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Search\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# search in the index\n",
    "def searchQueryGen(input_array=X_TEST):\n",
    "    for i, vec in enumerate(input_array):\n",
    "        yield {\n",
    "            \"_source\": False, # Don't get the source as this impacts latency\n",
    "            \"size\": 100,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"vec\": {\n",
    "                        \"vector\": vec.tolist(),\n",
    "                        \"k\": 100\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "neighbors_lists = []\n",
    "search_latency = []\n",
    "took_time = []\n",
    "for query in tqdm(searchQueryGen(), total=len(X_TEST)):\n",
    "    start = time.time()\n",
    "    search_response = client.search(body=query, index=vector_index_name, _source=False, docvalue_fields=[\"_id\"], stored_fields=\"_none_\")\n",
    "    end = time.time()\n",
    "    search_latency.append(end - start)\n",
    "    took_time.append(search_response[\"took\"])\n",
    "    search_hits = search_response['hits']['hits']\n",
    "    search_neighbors = [int(hit[\"fields\"][\"_id\"][0]) for hit in search_hits]\n",
    "    neighbors_lists.append(search_neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Metrics\n",
    "print(\"========================== Search Metrics ===================================\")\n",
    "print(\"========================== Server Side Latency ===================================\")\n",
    "print(f\"average took_time(ms): {np.average(took_time)}\") \n",
    "print(f\"p50 took_time(ms): {np.percentile(took_time, 50)}\") \n",
    "print(f\"p90 took_time(ms): {np.percentile(took_time, 90)}\")\n",
    "print(f\"p90 took_time(ms): {np.percentile(took_time, 99)}\")\n",
    "\n",
    "\n",
    "print(\"========================== Client side latency ===================================\")\n",
    "print(f\"\\n\\naverage Latency(ms): {np.average(search_latency) *1000}\") \n",
    "print(f\"p50 Latency(ms): {np.percentile(search_latency, 50) *1000}\") \n",
    "print(f\"p90 Latency(ms): {np.percentile(search_latency, 90) *1000}\")\n",
    "print(f\"p99 Latency(ms): {np.percentile(search_latency, 99) *1000}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"took_time\": took_time\n",
    "})\n",
    "\n",
    "avg_latency = sum(took_time) / len(took_time)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['took_time'], label=\"took_time\", color='blue', s=5)\n",
    "plt.title(f\"Search took_time | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(X_TEST)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 30) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"took_time (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"search_latency\": [(lat*1000) for lat in search_latency]\n",
    "})\n",
    "\n",
    "avg_latency = sum([(lat*1000) for lat in search_latency]) / len(search_latency)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['search_latency'], label=\"search_latency\", color='blue', s=5)\n",
    "plt.title(f\"Search search_latency | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(X_TEST)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 300) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"search_latency (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization for Ingest\n",
    "import multiprocessing.pool\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from opensearchpy.helpers import bulk\n",
    "\n",
    "def ingest_data(client_number, vectors_to_be_ingested, start_id):\n",
    "    print(f\"Starting ingest for client {client_number} for vectors counts: {len(vectors_to_be_ingested)}\")\n",
    "    def dataGenLocal():\n",
    "        for i, vec in enumerate(vectors_to_be_ingested):\n",
    "            yield { \"_index\": vector_index_name, \"_id\": str(start_id + i + 1), \"vec\": vec.tolist() }\n",
    "\n",
    "    bulk_size = 1000\n",
    "    data_to_ingest = []\n",
    "    total_time_to_ingest = 0.\n",
    "    ingest_latency = []\n",
    "    for data in tqdm(dataGenLocal(), total=len(vectors_to_be_ingested)):\n",
    "        if len(data_to_ingest) == bulk_size:\n",
    "            start = time.time()\n",
    "            (res, errors) = bulk(client, data_to_ingest)\n",
    "            end = time.time()\n",
    "            total_time_to_ingest += (end-start)\n",
    "            ingest_latency.append(end-start)\n",
    "            if len(errors) != 0:\n",
    "                print(errors)\n",
    "                data_to_ingest = []\n",
    "\n",
    "                StopIteration\n",
    "            else:\n",
    "                data_to_ingest = []\n",
    "\n",
    "        if len(data_to_ingest) < bulk_size:\n",
    "            data_to_ingest.append(data)\n",
    "        \n",
    "\n",
    "    if len(data_to_ingest) != 0:\n",
    "        start = time.time()\n",
    "        (res, errors) = bulk(client, data_to_ingest)\n",
    "        end = time.time()\n",
    "        total_time_to_ingest += (end-start)\n",
    "        if len(errors) != 0:\n",
    "            print(errors)\n",
    "        else:\n",
    "            data_to_ingest = []\n",
    "    return total_time_to_ingest, ingest_latency, errors\n",
    "    \n",
    "    \n",
    "\n",
    "create_index(vector_index_name)\n",
    "\n",
    "clients = 10 # change this to whatever value is suited for you\n",
    "\n",
    "vectors_per_client = len(X_TRAIN)//clients\n",
    "\n",
    "print(f\"Total Vectors are: {len(X_TEST)}, vectors per client is : {vectors_per_client}\")\n",
    "\n",
    "\n",
    "vectors = []\n",
    "batch = []\n",
    "i = 0\n",
    "print(f\"Partioning the data...\")\n",
    "for vector in X_TRAIN:\n",
    "    i = i + 1\n",
    "    if len(batch) == vectors_per_client:\n",
    "        vectors.append(batch)\n",
    "        batch = []\n",
    "    batch.append(vector)\n",
    "# Put the last batch of vectors\n",
    "vectors.append(batch)\n",
    "\n",
    "mp_total_time_to_ingest = 0.\n",
    "mp_ingest_latency = []\n",
    "mp_errors = []\n",
    "ctx = multiprocessing.get_context(\"fork\")\n",
    "with ctx.Pool(clients) as pool:\n",
    "    ingest_arg_tuple = [(idx + 1, vectors[idx], idx * vectors_per_client) for idx in range(clients)]\n",
    "    ingestion_response = list(pool.starmap(ingest_data, ingest_arg_tuple))\n",
    "for future in ingestion_response:\n",
    "    t_time_to_ingest, i_latency, errors = future\n",
    "    mp_total_time_to_ingest += t_time_to_ingest\n",
    "    mp_ingest_latency.extend(i_latency)\n",
    "    mp_errors.extend(errors)\n",
    "\n",
    "if len(mp_errors) !=0 :\n",
    "    print(f\"Errors happened while doing the ingest. {errors}\")\n",
    "else:\n",
    "    print(f\"Total time to ingest documents are : {mp_total_time_to_ingest} for documents : {len(X_TEST)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Metrics For Multi Processing Ingestion\n",
    "print(\"========================== Ingest Metrics Multi Processing ===================================\")\n",
    "\n",
    "print(\"========================== Client side latency ===================================\")\n",
    "print(f\"\\n\\naverage Latency(ms): {np.average(mp_ingest_latency) *1000} for bulksize: {bulk_size}\") \n",
    "print(f\"p50 Latency(ms): {np.percentile(mp_ingest_latency, 50) *1000} for bulksize: {bulk_size}\") \n",
    "print(f\"p90 Latency(ms): {np.percentile(mp_ingest_latency, 90) *1000} for bulksize: {bulk_size}\")\n",
    "print(f\"p99 Latency(ms): {np.percentile(mp_ingest_latency, 99) *1000} for bulksize: {bulk_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"mp_ingest_latency\": [(lat*1000) for lat in mp_ingest_latency]\n",
    "})\n",
    "\n",
    "avg_latency = sum([(lat*1000) for lat in mp_ingest_latency]) / len(mp_ingest_latency)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['mp_ingest_latency'], label=\"mp_ingest_latency\", color='blue', s=5)\n",
    "plt.title(f\"Ingest mp_ingest_latency | avg {avg_latency:.2f} ms for bulk size : {bulk_size}\")\n",
    "plt.xlabel(\"Ingest Run\")\n",
    "plt.xlim(0, len(X_TRAIN)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 1000) \n",
    "plt.ylabel(\"mp_ingest_latency (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization for Search\n",
    "import multiprocessing.pool\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "def run_search(client_number, query_list):\n",
    "    neighbors_lists = []\n",
    "    search_latency = []\n",
    "    took_time = []\n",
    "    print(f\"client_number : {client_number}, query: {len(query_list)}\")\n",
    "    # Better to create client for each process so that no locking happens\n",
    "    c = OpenSearch(\n",
    "        hosts = [{'host': OS_HOST, 'port': OS_PORT}],\n",
    "        http_compress = True, # enables gzip compression for request bodies\n",
    "        http_auth = (OS_USER, OS_PASSWORD),\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection,\n",
    "        timeout=6000,\n",
    "        pool_maxsize = 20\n",
    "    )\n",
    "    # Doing an info call to make sure client is ready, this will avoid the latency for 1st connection\n",
    "    c.info()\n",
    "    \n",
    "    for query in tqdm(query_list):\n",
    "        query_body = {\n",
    "            \"size\": 100,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"vec\": {\n",
    "                        \"vector\": query.tolist(),\n",
    "                        \"k\": 100\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        start = time.time()\n",
    "        search_response = c.search(body=query_body, index=vector_index_name, _source=False, docvalue_fields=[\"_id\"], stored_fields=\"_none_\")\n",
    "        end = time.time()\n",
    "        search_latency.append(end - start)\n",
    "        took_time.append(search_response[\"took\"])\n",
    "        search_hits = search_response['hits']['hits']\n",
    "        search_neighbors = [int(hit[\"fields\"][\"_id\"][0]) for hit in search_hits]\n",
    "        neighbors_lists.append(search_neighbors)\n",
    "    return search_latency, took_time, neighbors_lists\n",
    "        \n",
    "\n",
    "clients = 10\n",
    "\n",
    "queries_per_client = len(X_TEST)//clients\n",
    "\n",
    "print(f\"Total Queries are: {len(X_TEST)}, queries per client is : {queries_per_client}\")\n",
    "\n",
    "\n",
    "queries = []\n",
    "batch = []\n",
    "i = 0\n",
    "for query in X_TEST:\n",
    "    i = i + 1\n",
    "    if len(batch) == queries_per_client:\n",
    "        queries.append(batch)\n",
    "        batch = []\n",
    "    batch.append(query)\n",
    "# Put the last batch of queries\n",
    "queries.append(batch)\n",
    "\n",
    "mp_neighbors_lists = []\n",
    "mp_search_latency = []\n",
    "mp_took_time = []\n",
    "\n",
    "ctx = multiprocessing.get_context(\"fork\")\n",
    "with ctx.Pool(clients) as pool:\n",
    "    query_arg_tuple = [(idx + 1, queries[idx]) for idx in range(clients)]\n",
    "    queries_response = list(pool.starmap(run_search, query_arg_tuple))\n",
    "for future in queries_response:\n",
    "    s_lat, t_time, n_lists = future\n",
    "    mp_search_latency.extend(s_lat)\n",
    "    mp_neighbors_lists.extend(n_lists)\n",
    "    mp_took_time.extend(t_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Metrics For Multi Processing\n",
    "print(\"========================== Search Metrics Multi Processing ===================================\")\n",
    "print(\"========================== Server Side Latency ===================================\")\n",
    "print(f\"average took_time(ms): {np.average(mp_took_time)}\") \n",
    "print(f\"p50 took_time(ms): {np.percentile(mp_took_time, 50)}\") \n",
    "print(f\"p90 took_time(ms): {np.percentile(mp_took_time, 90)}\")\n",
    "print(f\"p90 took_time(ms): {np.percentile(mp_took_time, 99)}\")\n",
    "\n",
    "\n",
    "print(\"========================== Client side latency ===================================\")\n",
    "print(f\"\\n\\naverage Latency(ms): {np.average(mp_search_latency) *1000}\") \n",
    "print(f\"p50 Latency(ms): {np.percentile(mp_search_latency, 50) *1000}\") \n",
    "print(f\"p90 Latency(ms): {np.percentile(mp_search_latency, 90) *1000}\")\n",
    "print(f\"p99 Latency(ms): {np.percentile(mp_search_latency, 99) *1000}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"mp_took_time\": mp_took_time\n",
    "})\n",
    "\n",
    "avg_latency = sum(mp_took_time) / len(mp_took_time)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['mp_took_time'], label=\"mp_took_time\", color='blue', s=5)\n",
    "plt.title(f\"Search mp_took_time | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(X_TEST)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 30) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"mp_took_time (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"mp_search_latency\": [(lat*1000) for lat in mp_search_latency]\n",
    "})\n",
    "\n",
    "avg_latency = sum([(lat*1000) for lat in mp_search_latency]) / len(mp_search_latency)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['mp_search_latency'], label=\"mp_search_latency\", color='blue', s=5)\n",
    "plt.title(f\"Search mp_search_latency | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(X_TEST)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 100) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"mp_search_latency (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
