{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Require python 3.10+\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir opensearch-py python-dotenv boto3 tqdm h5py matplotlib ipywidgets jedi ipython sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autocomplete use shift+tab\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a dataset Scifact\n",
    "\n",
    "!curl -o scifact.zip -L https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/scifact.zip\n",
    "!unzip scifact.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read Data set\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "corpus_file = \"./scifact/corpus.jsonl\"\n",
    "queries_file = \"./scifact/queries.jsonl\"\n",
    "\n",
    "num_lines = sum(1 for i in open(corpus_file, 'rb'))\n",
    "corpus = {}\n",
    "queries = {}\n",
    "print(f\"Loading dataset... \")\n",
    "with open(corpus_file, encoding='utf8') as fIn:\n",
    "    for line in tqdm(fIn, total=num_lines):\n",
    "        line = json.loads(line)\n",
    "        corpus[line.get(\"_id\")] = {\n",
    "            \"text\": line.get(\"text\"),\n",
    "            \"title\": line.get(\"title\"),\n",
    "        }\n",
    "\n",
    "print(f\"Dataset size is : {num_lines}\")\n",
    "\n",
    "print(f\"Loading queries... \")\n",
    "num_lines = sum(1 for i in open(queries_file, 'rb'))\n",
    "queries = {}\n",
    "\n",
    "with open(queries_file, encoding='utf8') as fIn:\n",
    "    for line in tqdm(fIn, total=num_lines):\n",
    "        line = json.loads(line)\n",
    "        queries[line.get(\"_id\")] = { \n",
    "            \"text\": line.get(\"text\")\n",
    "        }\n",
    "\n",
    "\n",
    "print(f\"Queries size is : {num_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sentence Transformer model Example\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "print(model)\n",
    "dimension = model.get_sentence_embedding_dimension()\n",
    "print(f\"Model dimension is : {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "\n",
    "\n",
    "res = load_dotenv(\"environment.txt\")\n",
    "\n",
    "OS_HOST = os.getenv('OS_HOST')\n",
    "OS_PORT = os.getenv('OS_PORT')\n",
    "OS_USER = os.getenv('USER_NAME')\n",
    "OS_PASSWORD = os.getenv('PASSWORD')\n",
    "\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': OS_HOST, 'port': OS_PORT}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (OS_USER, OS_PASSWORD),\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    timeout=6000,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "client.info()\n",
    "hybrid_search_index_name = os.getenv('HYBRID_SEARCH_INDEX_NAME', \"hybrid_search_index\")\n",
    "print(f\"hybrid search index name from env is : {hybrid_search_index_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_index(index_name, dimension):\n",
    "    index_mappings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            #\"refresh_interval\": \"-1\",\n",
    "            \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 128 # Adjust to improve precision. Higher improves recall & precsion but increases latency. Lower degrades recall & precision but improves latency.\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"vec\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": dimension,\n",
    "                    \"index\": \"true\",\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"space_type\": \"l2\", # l2 for SIFT, cosinesimil for typical\n",
    "                        \"engine\": \"nmslib\",\n",
    "                        \"parameters\": {\n",
    "                            \"ef_construction\": 128\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if client.indices.exists(index=index_name):\n",
    "        response = client.indices.delete(index=index_name)\n",
    "        print(f\"Deleting the index. Response : {response}\")\n",
    "\n",
    "    response = client.indices.create(index=index_name, body=index_mappings)\n",
    "    print(f\"Creating the index. Response : {response}\")\n",
    "\n",
    "create_index(hybrid_search_index_name, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the index as we set the refresh interval to -1\n",
    "client.indices.refresh(index=hybrid_search_index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus into embeddings\n",
    "# This cell can takes a lot of time to run.\n",
    "for k,v in corpus.items():\n",
    "    vector = model.encode(v['text'])\n",
    "    v['vector'] = vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert queries to vectors\n",
    "for k , v in queries.items():\n",
    "    vector = model.encode(v['text'])\n",
    "    v['vector'] = vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the vectors for queries\n",
    "limit = 2\n",
    "for index, v in zip(range(limit), queries.items()):\n",
    "    print(v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 2 records.\n",
    "\n",
    "limit = 2\n",
    "for index, v in zip(range(limit), corpus.items()):\n",
    "    print(v[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest data in the index\n",
    "import time\n",
    "from opensearchpy.helpers import bulk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for i, vec in enumerate(X_TRAIN):\n",
    "#         yield { \"_index\": vector_index_name, \"_id\": str(i + 1), \"vec\": vec.tolist() }\n",
    "\n",
    "def index_data_gen(corpus):\n",
    "    for key, value in corpus.items():\n",
    "        yield { \"_index\": hybrid_search_index_name, \"_id\": str(key), \"vec\": value[\"vector\"], \"text\": value['text'], \"title\": value['title'] }\n",
    "\n",
    "bulk_size = 100\n",
    "\n",
    "total_time_to_ingest = 0.\n",
    "ingest_latency = []\n",
    "data_to_ingest = []\n",
    "\n",
    "for data in tqdm(index_data_gen(corpus), total=len(corpus)):\n",
    "    if len(data_to_ingest) == bulk_size:\n",
    "        start = time.time()\n",
    "        (res, errors) = bulk(client, data_to_ingest)\n",
    "        end = time.time()\n",
    "        total_time_to_ingest += (end-start)\n",
    "        ingest_latency.append(end-start)\n",
    "        if len(errors) != 0:\n",
    "            print(errors)\n",
    "            data_to_ingest = []\n",
    "            StopIteration\n",
    "        else:\n",
    "            data_to_ingest = []\n",
    "\n",
    "    if len(data_to_ingest) < bulk_size:\n",
    "        data_to_ingest.append(data)\n",
    "\n",
    "\n",
    "if len(data_to_ingest) != 0:\n",
    "    start = time.time()\n",
    "    (_, errors) = bulk(client, data_to_ingest)\n",
    "    end = time.time()\n",
    "    total_time_to_ingest += (end-start)\n",
    "    if len(errors) != 0:\n",
    "        print(errors)\n",
    "    else:\n",
    "        data_to_ingest = []\n",
    "\n",
    "print(f\"Ingestion completed. Total time to ingest = {total_time_to_ingest} seconds, average time per document: {total_time_to_ingest/(len(corpus))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check index details, you should see 1M documents in the index.\n",
    "print(client.cat.indices(index=hybrid_search_index_name))\n",
    "\n",
    "print(\"Segments Info After refresh...\")\n",
    "\n",
    "segments = client.cat.segments(hybrid_search_index_name, params={\"format\": \"json\"})\n",
    "\n",
    "print(f\"Total segments are: {len(segments)}\")\n",
    "\n",
    "print(f\"Printing Segment info : \\n{client.cat.segments(index=hybrid_search_index_name, params={'format': 'csv', 'v': 'true'})}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do the hybrid query now using Bool Query clause\n",
    "limit = 2\n",
    "for index, v in zip(range(limit), queries.items()):\n",
    "    print(v[1])\n",
    "\n",
    "\n",
    "def generate_query_clause(queries):\n",
    "    for _, query in queries.items():\n",
    "        query_body = {\n",
    "            \"size\": 10,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\n",
    "                            \"match\": {\n",
    "                                \"text\": query['text']\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"knn\": {\n",
    "                                \"vec\": {\n",
    "                                    \"vector\": query['vector'],\n",
    "                                    \"k\": 10\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        yield query_body\n",
    "\n",
    "\n",
    "search_latency = []\n",
    "took_time = []\n",
    "\n",
    "for query_body in tqdm(generate_query_clause(queries), total=len(queries)):\n",
    "    start = time.time()\n",
    "    search_response = client.search(body=query_body, index=hybrid_search_index_name, _source=False, docvalue_fields=[\"_id\"], stored_fields=\"_none_\")\n",
    "    end = time.time()\n",
    "    search_latency.append(end - start)\n",
    "    took_time.append(search_response[\"took\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"========================== Search Metrics ===================================\")\n",
    "print(\"\\n\\n========================== Server Side Latency ===================================\")\n",
    "print(f\"average took_time(ms): {np.average(took_time)}\") \n",
    "print(f\"p50 took_time(ms): {np.percentile(took_time, 50)}\") \n",
    "print(f\"p90 took_time(ms): {np.percentile(took_time, 90)}\")\n",
    "print(f\"p90 took_time(ms): {np.percentile(took_time, 99)}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n========================== Client side latency ===================================\")\n",
    "print(f\"average Latency(ms): {np.average(search_latency) *1000}\") \n",
    "print(f\"p50 Latency(ms): {np.percentile(search_latency, 50) *1000}\") \n",
    "print(f\"p90 Latency(ms): {np.percentile(search_latency, 90) *1000}\")\n",
    "print(f\"p99 Latency(ms): {np.percentile(search_latency, 99) *1000}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"took_time\": took_time\n",
    "})\n",
    "\n",
    "avg_latency = sum(took_time) / len(took_time)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['took_time'], label=\"took_time\", color='blue', s=5)\n",
    "plt.title(f\"Search took_time | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(queries)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 30) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"took_time (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"search_latency\": [(lat*1000) for lat in search_latency]\n",
    "})\n",
    "\n",
    "avg_latency = sum([(lat*1000) for lat in search_latency]) / len(search_latency)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.scatter(df.index, df['search_latency'], label=\"search_latency\", color='blue', s=5)\n",
    "plt.title(f\"Search search_latency | avg {avg_latency:.2f} ms\")\n",
    "plt.xlabel(\"Search Run\")\n",
    "plt.xlim(0, len(queries)) # change this value to view a wider range of data. (Search Run Batch)\n",
    "plt.ylim(0, 400) # change this value to view the precision/recall, some values may be out of view.\n",
    "plt.ylabel(\"search_latency (milliseconds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
